<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>PII: S0925-2312(00)00293-9</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.36">
<META name="date" content="2000-10-19T11:13:24+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<A name=1></a>Neurocomputing 35 (2000) 3}26<br>
Class separability estimation and incremental learning using<br>
boundary methods<br>
JoseH-Luis Sancho *, William E. Pierson , Batu Ulug ,<br>
AnmHbal R. Figueiras-Vidal , Stanley C. Ahalt <br>
<i>ATSC-DI, Escuela Polite</i>&amp;<i>cnica Superior. Uni</i>v<i>ersidad Carlos III Legane</i>&amp;<i>s-Madrid, Spain</i><br>
<i>Department of Electrical Engineering, The Ohio State Uni</i>v<i>ersity Columbus, OH 43210, USA</i><br>
Received 7 January 1999; revised 5 April 1999; accepted 10 April 2000<br>
Abstract<br>
In this paper we discuss the use of boundary methods (BMs) for distribution analysis. We<br>
view these methods as tools which can be used to extract useful information from sample<br>distributions. We believe that the information thus extracted has utility for a number of<br>applications, but in particular we discuss the use of BMs as a mechanism for class separability<br>estimation and as an aid to constructing robust and e$cient neural networks (NNs) to solve<br>classi&quot;cation problems. In the &quot;rst case, BMs can establish the utility of a data set for<br>classi&quot;cation. We demonstrate experimentally that the derived ranking is consistent with<br>alternative ranking techniques based on Bayes error ( ). Finally, BMs are used as sample<br>selection (SS) mechanism to train NN by means of gradient algorithms. In particular, elliptic<br>BMs (EBMs) are used to select samples so that the initial partial training set is linearly<br>separable. In a progressive way, new samples are added to the training set solving the problem<br>in an incremental manner. Multi-layer perceptrons (MLPs) and radial basis functions (RBFs)<br>have been used in this work. Our results show that the probability of being trapped in a local<br>minimum is clearly reduced when EBMs are used, making the training independent of the<br>initial weight values. Also, the e!ect of the very noisy samples and outliers is reduced when the<br>SS-EBM algorithm is employed, so we propose this method as a robust procedure to train NNs<br>by means of a gradient learning rule.<br>
2000 Elsevier Science B.V. All rights reserved.<br>
<i>Keywords: </i>Classi&quot;cation neural systems; Class separability estimation; Bayes' error estimation;<br>Gradient algorithms; Sample selection strategies<br>
* Corresponding author. Tel.: #34-91-6249173; fax: #34-91-62-49430.<br><i>E-mail address: </i>jlsancho@tsc.uc3m.es (J.-L. Sancho).<br> JoseH-Luis Sancho and AnmHbal R. Figueiras-Vidal were partially supported by grant from CICYT<br>
Project TIC96-0500-C10-03.<br>
 Batu Ulug and Stan Ahalt were partially supported by grants from the Air Force O$ce of Scienti&quot;c<br>
Research and the U.S. Army Research Lab Programming Environment and Training program.<br>
0925-2312/00/$ - see front matter<br>
2000 Elsevier Science B.V. All rights reserved.<br>
PII: S 0 9 2 5 - 2 3 1 2 ( 0 0 ) 0 0 2 9 3 - 9<br>
<hr>
</BODY>
</HTML>
